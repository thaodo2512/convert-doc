import yaml
import json
import struct
import argparse
import sys
import os
import glob
from datetime import datetime

# Mapping PLDM/YAML types to Python struct formats (Little Endian)
TYPE_MAP = {
    'uint8': 'B', 'int8': 'b', 'enum8': 'B', 'bitfield8': 'B', 'bool': '?',
    'uint16': '<H', 'int16': '<h',
    'uint32': '<I', 'int32': '<i',
    'float': '<f', 'real32': '<f',
    'uint64': '<Q', 'int64': '<q',
    'uuid': 'B'
}

AUTO_TOKENS = {"auto", "auto gen", "auto generated", "autogen", "autogenerated", "auto-gen", "auto_gen"}
DOC_META_KEYS = {"docHidden", "_doc_hidden", "_docHide", "_doc_hidden", "_doc"}
HEADER_FIELD_TYPES = {
    'recordHandle': 'uint32',
    'PDRHeaderVersion': 'uint8',
    'PDRType': 'uint8',
    'recordChangeNumber': 'uint16',
    'dataLength': 'uint16',
}


def normalize_auto(val):
    return " ".join(str(val).lower().replace("-", " ").replace("_", " ").split())


def is_auto_value(val):
    if val is None:
        return True
    if isinstance(val, str) and normalize_auto(val) in AUTO_TOKENS:
        return True
    return False


def extract_value(node):
    if isinstance(node, dict) and 'value' in node:
        return node['value']
    return node


def coerce_int(val, field, filename):
    try:
        if isinstance(val, str):
            return int(val, 0)
        return int(val)
    except (TypeError, ValueError):
        print(f"Error {filename}: '{field}' must be an integer or 'auto', got {val!r}")
        sys.exit(1)


def next_available_handle(used_handles, reserved_handles, start):
    candidate = start
    while candidate in used_handles or candidate in reserved_handles:
        candidate += 1
    return candidate


def collect_reserved_handles(yaml_files):
    reserved = set()
    duplicates = set()
    for path in yaml_files:
        try:
            with open(path, 'r') as f:
                data = yaml.safe_load(f) or {}
        except Exception as exc:
            print(f"Error reading {path}: {exc}")
            sys.exit(1)
        header = data.get('pdrHeader', {}) or {}
        raw_handle = extract_value(header.get('recordHandle'))
        if raw_handle is None or is_auto_value(raw_handle):
            continue
        handle = coerce_int(raw_handle, "recordHandle", path)
        if handle in reserved:
            duplicates.add(handle)
        reserved.add(handle)
    return reserved, duplicates


def get_value_and_type(node):
    if isinstance(node, dict):
        if 'value' in node:
            return node['value'], node.get('type', 'uint8')
        else:
            return node, 'struct'
    return node, 'uint8'

def pack_simple(val, type_name):
    try:
        if isinstance(val, list):
            fmt = TYPE_MAP.get(type_name, 'B')
            packed = bytearray()
            for item in val:
                packed.extend(struct.pack(fmt, item))
            return packed
        if type_name == 'bool':
            return struct.pack('?', bool(val))
        fmt = TYPE_MAP.get(type_name, 'B')
        return struct.pack(fmt, val)
    except Exception as e:
        print(f"ERROR: Packing value '{val}' as '{type_name}': {e}")
        sys.exit(1)

def pack_node(node):
    val, dtype = get_value_and_type(node)
    if dtype == 'struct':
        packed = bytearray()
        for key in val:
            if key in DOC_META_KEYS:
                continue
            packed.extend(pack_node(val[key]))
        return packed
    else:
        return pack_simple(val, dtype)

def process_single_yaml(yaml_path, schema_dir, used_handles, reserved_handles, next_handle):
    filename = os.path.basename(yaml_path)
    var_name = os.path.splitext(filename)[0].replace('-', '_').replace(' ', '_')
    
    with open(yaml_path, 'r') as f:
        data = yaml.safe_load(f)

    pdr_header = data.get('pdrHeader') or {}
    if not pdr_header:
        print(f"Skipping {filename}: Could not find 'pdrHeader'")
        return None, None, next_handle

    # 1. Determine PDR Type to find the Schema
    pdr_type_raw = extract_value(pdr_header.get('PDRType'))
    if pdr_type_raw is None:
        print(f"Skipping {filename}: Could not find 'pdrHeader.PDRType'")
        return None, None, next_handle
    pdr_type = coerce_int(pdr_type_raw, "PDRType", filename)

    schema_filename = f"type_{pdr_type}.json"
    schema_path = os.path.join(schema_dir, schema_filename)

    if not os.path.exists(schema_path):
        print(f"Error processing {filename}: Schema '{schema_filename}' not found in {schema_dir}")
        sys.exit(1)

    with open(schema_path, 'r') as f:
        schema = json.load(f)

    # 2. Pack body
    body_buffer = bytearray()
    if 'binaryOrder' not in schema:
        print(f"Error {filename}: Schema {schema_filename} missing 'binaryOrder'")
        sys.exit(1)
    for field in schema['binaryOrder']:
        if field in data:
            body_buffer.extend(pack_node(data[field]))
        else:
            print(f"Error {filename}: Missing field '{field}' in data")
            sys.exit(1)

    computed_data_length = len(body_buffer)
    if computed_data_length > 0xFFFF:
        print(f"Error {filename}: dataLength {computed_data_length} exceeds uint16 range")
        sys.exit(1)

    # 3. Resolve recordHandle (auto-generate if missing/auto)
    raw_handle = extract_value(pdr_header.get('recordHandle'))
    if is_auto_value(raw_handle):
        handle = next_available_handle(used_handles, reserved_handles, next_handle)
        used_handles.add(handle)
        next_handle = next_available_handle(used_handles, reserved_handles, handle + 1)
        print(f"  - Auto-assigned recordHandle {handle} for {filename}")
    else:
        handle = coerce_int(raw_handle, "recordHandle", filename)
        if handle in used_handles:
            new_handle = next_available_handle(used_handles, reserved_handles, max(next_handle, handle))
            print(f"  - recordHandle {handle} already used; reassigned to {new_handle} for {filename}")
            handle = new_handle
        used_handles.add(handle)
        next_handle = next_available_handle(used_handles, reserved_handles, max(next_handle, handle + 1))

    if handle < 0 or handle > 0xFFFFFFFF:
        print(f"Error {filename}: recordHandle {handle} is outside uint32 range")
        sys.exit(1)

    # 4. Build header with computed dataLength
    header_fields = {}
    for field in ['PDRHeaderVersion', 'PDRType', 'recordChangeNumber']:
        if field not in pdr_header:
            print(f"Error {filename}: Missing header field '{field}'")
            sys.exit(1)
        header_fields[field] = coerce_int(extract_value(pdr_header[field]), field, filename)

    provided_length = extract_value(pdr_header.get('dataLength'))
    if provided_length is not None and not is_auto_value(provided_length):
        provided_int = coerce_int(provided_length, "dataLength", filename)
        if provided_int != computed_data_length:
            print(f"  - Adjusted dataLength for {filename}: expected {computed_data_length} (was {provided_int})")

    header_fields['recordHandle'] = handle
    header_fields['dataLength'] = computed_data_length

    header_buffer = bytearray()
    for field in ['recordHandle', 'PDRHeaderVersion', 'PDRType', 'recordChangeNumber', 'dataLength']:
        header_buffer.extend(pack_simple(header_fields[field], HEADER_FIELD_TYPES[field]))

    byte_buffer = header_buffer + body_buffer
    print(f"  - Processed {filename} (Type {pdr_type}) -> {len(byte_buffer)} bytes (dataLength {computed_data_length})")
    return byte_buffer, var_name, next_handle

def generate_all(yaml_dir, schema_dir, output_file):
    if not os.path.exists(yaml_dir):
        print(f"YAML Directory not found: {yaml_dir}")
        sys.exit(1)

    yaml_files = glob.glob(os.path.join(yaml_dir, "*.yaml"))
    if not yaml_files:
        print("No .yaml files found in directory.")
        sys.exit(0)

    print(f"Found {len(yaml_files)} YAML files in {yaml_dir}")

    reserved_handles, duplicate_handles = collect_reserved_handles(yaml_files)
    if duplicate_handles:
        print(f"Warning: Duplicate recordHandle values detected (will auto-renumber later occurrences): {sorted(duplicate_handles)}")
    used_handles = set()
    next_handle = next_available_handle(used_handles, reserved_handles, 1)

    generated_arrays = [] # List of (var_name, size)

    # Start C File Content
    c_source = [
        "/*",
        f" * Generated by code_gen_dir.py",
        f" * Timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
        " */",
        "",
        "#include <stdint.h>",
        "#include <stddef.h>",
        ""
    ]

    # Process each file
    for yaml_file in sorted(yaml_files):
        byte_data, var_name, next_handle = process_single_yaml(
            yaml_file, schema_dir, used_handles, reserved_handles, next_handle
        )
        if byte_data:
            # Add array definition
            c_source.append(f"// Source: {os.path.basename(yaml_file)}")
            c_source.append(f"const uint8_t {var_name}[] = {{")
            
            # Hex formatting
            hex_lines = []
            for i in range(0, len(byte_data), 12):
                chunk = byte_data[i:i + 12]
                hex_lines.append("    " + ", ".join(f"0x{b:02X}" for b in chunk) + ",")
            
            # Remove last comma for C strictness
            if hex_lines: hex_lines[-1] = hex_lines[-1].rstrip(',')
            
            c_source.extend(hex_lines)
            c_source.append("};")
            c_source.append(f"const size_t {var_name}_size = sizeof({var_name});")
            c_source.append("")
            
            generated_arrays.append(var_name)

    # Generate Registry (Array of Pointers)
    # This allows C code to loop through all PDRs without knowing their names
    c_source.append("/* --- PDR Registry --- */")
    c_source.append("typedef struct {")
    c_source.append("    const uint8_t* data;")
    c_source.append("    size_t size;")
    c_source.append("} pdr_entry_t;")
    c_source.append("")
    c_source.append("const pdr_entry_t pdr_registry[] = {")
    
    for name in generated_arrays:
        c_source.append(f"    {{ {name}, {name}_size }},")
    
    c_source.append("};")
    c_source.append("")
    c_source.append(f"const size_t pdr_registry_count = {len(generated_arrays)};")

    # Write output
    output_dir = os.path.dirname(output_file)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir)

    with open(output_file, 'w') as f:
        f.write("\n".join(c_source))

    print(f"Success! Generated {output_file} containing {len(generated_arrays)} PDRs.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Generate C code from directory of PLDM YAMLs")
    parser.add_argument("yaml_dir", help="Directory containing .yaml files")
    parser.add_argument("schema_dir", help="Directory containing .json schema files")
    parser.add_argument("out", help="Output .c file path")

    args = parser.parse_args()
    generate_all(args.yaml_dir, args.schema_dir, args.out)
